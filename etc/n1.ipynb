{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e29845e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FRIDAY 1: TOKENIZATION & EMBEDDINGS \n",
    "# components \n",
    "# 1. Tokenizer Convertes text <-> Numbers\n",
    "# 2. Token Embedding - Numbers -> Dense vectors \n",
    "# 3. Positional embeddings - Add position information \n",
    "# 4. Testing & Visualization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52a0ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import os \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86823e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्कार! आज म बिहान ७:३० बजे उठेँ। मौसम सुन्दर छ—घाम चम्किरहेको छ। मैले चिया बनाएँ र समाचार पढें। स्कूल जानुअघि किताबहरू (गणित, विज्ञान, साहित्य) र कापी, कलम, पेन्सिल तयारी गर्नुपर्नेछ। हिजोका कार्यहरू, ईमेलहरू, र फेसबुक/इन्स्टाग्राम जाँच्न पनि समय लिनुपर्\\u200dयो। मैले सँगै @राम, #सुनिता, र मित्रहरूसँग कुरा गरें। उनीहरूले भने: “हामी क्याफेमा १०:०० बजे भेट्नेछौं।” त्यसपछि म घर फर्केर टेलिफोन नम्बर ०१-४४५५६७८९ मा सम्पर्क गरें। आजको दिन धेरै व्यस्त छ; तर काम पूरा गर्दा खुशी लाग्छ।'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data \n",
    "\n",
    "with open('../data/sample.txt','r',encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d9fb194",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (376368874.py, line 68)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 68\u001b[0;36m\u001b[0m\n\u001b[0;31m    is  # decoding\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Character -Level tokenizer for Nepali text \n",
    "    what it does \n",
    "    - Builds a vocabulart for all uqnieu characters in the text\n",
    "    - Convert text to numbers(encoding) and vice versa \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text):\n",
    "        # process : extract->sort->create bidirectional mapping\n",
    "\n",
    "        # 1. get all unique characterrs from the text\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        # 2. count how many unqiue characters from the text \n",
    "\n",
    "        self.vocab_size = len(self.chars)\n",
    "        # encoding \n",
    "        self.stoi = {}\n",
    "        for i, char in enumerate(self.chars):  # go through each character with index \n",
    "            self.stoi[ch] = i  #  assign the index t the character \n",
    "        \n",
    "        self.itos = {}\n",
    "        for i, char in enumerate(self.chars): \n",
    "            self.iots[i] = char\n",
    "\n",
    "    def encode(self,text):\n",
    "        indices = []\n",
    "        for c in text:\n",
    "            indices.append(self.stoi[c])\n",
    "        return indices\n",
    "\n",
    "# component 1 : TOKENIZER \n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Character -Level tokenizer for Nepali text \n",
    "    what it does \n",
    "    - Builds a vocabulart for all uqnieu characters in the text\n",
    "    - Convert text to numbers(encoding) and vice versa \n",
    "    \"\"\"\n",
    "    def __init__(self,text):\n",
    "        # process : extract->sort->create bidirectional mapping\n",
    "\n",
    "        # 1. get all unique characterrs from the text\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        # 2. count how many unqiue characters from the text \n",
    "        self.vocab_size = len(self.chars)\n",
    "        # encoding \n",
    "        self.stoi = {} # start withh an empty dictionary \n",
    "        for i,ch in enumerate(self.chars): # go through each character with index \n",
    "            self.stoi[ch] = i # assign the index t the character \n",
    "        \n",
    "        # decoding \n",
    "        self.itos = {}\n",
    "        for i,ch in enumerate(self.chars):\n",
    "            self.itos[i] = ch \n",
    "        \n",
    "    def encode(self,text):\n",
    "        indices = []\n",
    "        for c in text:\n",
    "            indices.append(self.stoi[c])\n",
    "        return indices\n",
    "\n",
    "    def decode(self,indices):\n",
    "        chars = []\n",
    "        for i in indices:\n",
    "            chars.append(self.itos[i])\n",
    "        return ''.join(chars)\n",
    "is  # decoding \n",
    "        self.itos = {}\n",
    "        for i,ch in enumerate(self.chars):\n",
    "            self.itos[i] = ch \n",
    "necessary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0036d268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8217fb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([34, 39, 45, 56, 19, 47, 41], 'नमस्कार')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eg use case \n",
    "tokenizer = Tokenizer(text)\n",
    "tokenizer.encode('नमस्कार'), tokenizer.decode([34, 39, 45, 56, 19, 47, 41])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "180de271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# component 2: TOKEN EMBEDINGS \n",
    "class TokenEmbedding(nn.Module):\n",
    "    # Converts token indices to dense vector representaions.\n",
    "    # What it does \n",
    "        # - each character index -> D-dimenssional vector \n",
    "        # - vectors are learned during training \n",
    "        # - similar characters get similar vectors \n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        # args\n",
    "        # vocab_size = num of unique tokens(chars)\n",
    "        # embedding_dim : size of embedding vectors \n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # creates the embedding layer \n",
    "        # this is essentially a lookup table (matrix)\n",
    "\n",
    "\n",
    "    def forward(self, token_indices):\n",
    "        return self.embedding(token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eeeb5a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmponent 3 : POSITIONAL EMBedDINGS\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\" \n",
    "    adds position information to token embeddings.\n",
    "    why wee nedd this:\n",
    "        - transformers process all tokens in parallel \n",
    "        - without position info, \"hello world\" == \"world hello\"\n",
    "        - position embeddings tell model the order of characters \n",
    "\n",
    "    two approaches \n",
    "    1. Learned(what we use ): Position embeddings are trainable parameteres \n",
    "    2. Fixed (Sinusoidal) : Use sin/cos functions\n",
    "    each position gets a unqiue learned vector added to it\n",
    "    \"\"\"\n",
    "    def __init__(self, max_seq_len, embedding_dim):\n",
    "        # creates position mebedding matrix of shape( max_seq_len, embedding_dim)\n",
    "        # one learned vector for each possibility \n",
    "        super().__init__()\n",
    "        # store dimensions \n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "\n",
    "\n",
    "    def forward(self, token_embeddings):\n",
    "        #  1. Get sequence length from input\n",
    "        #  2. Create position indices: [0, 1, 2, ..., seq_len-1]\n",
    "        #  3. Look up position embeddings\n",
    "        #  4. Add to token embeddings (broadcasting handles batch dimension)\n",
    "\n",
    "    # check : sequence can't be longer than max_seq_len\n",
    "        if seq_len > self.max_seq_len:\n",
    "            raise ValueError(\n",
    "                f\"Sequence Length {seq_len} exceeds maximum {self.max_seq_len}\"\n",
    "            )\n",
    "        # device= ensures positions are on same device as input (CPU/GPU)\n",
    "        positions = torch.arange(seq_len, device = token_embeddings.device)\n",
    "        # Look up position embeddings\n",
    "        # Shape: (seq_len, embedding_dim)\n",
    "        pos_emb = self.embedding(positions)\n",
    "\n",
    "\n",
    "        # Add position embeddings to token embeddings\n",
    "        # Broadcasting: pos_emb (seq_len, emb_dim) is added to each batch\n",
    "        # Result shape: (batch_size, seq_len, embedding_dim)\n",
    "        return token_embeddings + pos_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d208ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# component  4: COMBINED EMBEDDING LAYER\n",
    "\n",
    "class NepaliEmbedding(nn.Module):\n",
    "    # token + position \n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len):\n",
    "        # Initialize combined embedding layer.\n",
    "        super().__init__()\n",
    "\n",
    "        # store configuration \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim \n",
    "        self.max_seq_len = max_seq_len \n",
    "\n",
    "        # create token embeding layer \n",
    "        self.token_emb = TokenEmbedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # create positiona embedding layer \n",
    "        self.pos_emb  = PositionalEmbedding(max_seq_len, embedding_dim)\n",
    "\n",
    "        # calcualate total paramteres \n",
    "        total_parms = (vocab_size * embedding_dim) + (max_seq_len * embedding_dim)\n",
    "    \n",
    "    def forward(self, token_indices):\n",
    "        # convert tokenn indices to embeddings with postion embedding \n",
    "\n",
    "        # 1. get token embedding \n",
    "        tok_emb = self.token_emb(token_indices)\n",
    "\n",
    "        # 2.Add positioal embedding\n",
    "        embeddings = self.pos_emb(tok_emb)\n",
    "        return embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38f32aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix PositionalEmbedding.forward (seq_len was not defined) and replace the model's pos_emb\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_seq_len, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "\n",
    "    def forward(self, token_embeddings):\n",
    "        # token_embeddings: (batch_size, seq_len, embedding_dim) or (seq_len, embedding_dim)\n",
    "        if token_embeddings.dim() == 3:\n",
    "            seq_len = token_embeddings.size(1)\n",
    "        elif token_embeddings.dim() == 2:\n",
    "            seq_len = token_embeddings.size(0)\n",
    "            token_embeddings = token_embeddings.unsqueeze(0)  # add batch dim for consistent addition\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected token_embeddings shape: {token_embeddings.shape}\")\n",
    "\n",
    "        if seq_len > self.max_seq_len:\n",
    "            raise ValueError(\n",
    "                f\"Sequence Length {seq_len} exceeds maximum {self.max_seq_len}\"\n",
    "            )\n",
    "\n",
    "        positions = torch.arange(seq_len, device=token_embeddings.device)\n",
    "        pos_emb = self.embedding(positions)  # (seq_len, embedding_dim)\n",
    "\n",
    "        # broadcasting will add pos_emb (seq_len, emb_dim) to token_embeddings (batch, seq_len, emb_dim)\n",
    "        return token_embeddings + pos_emb\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2f04e1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size :6\n",
      "chars :['त', 'न', 'म', 'स', 'े', '्']\n",
      "Text: नमस्ते--> Numbers[1, 2, 3, 5, 0, 4]\n",
      "Number: [1, 2, 3, 5, 0, 4]--> text : नमस्ते\n",
      "NepaliEmbedding(\n",
      "  (token_emb): TokenEmbedding(\n",
      "    (embedding): Embedding(6, 64)\n",
      "  )\n",
      "  (pos_emb): PositionalEmbedding(\n",
      "    (embedding): Embedding(128, 64)\n",
      "  )\n",
      ")\n",
      "Input shape: torch.Size([1, 6])\n",
      "Output shape: torch.Size([1, 6, 64])\n",
      "Output:tensor([[[-1.1404, -1.5804, -0.5357,  0.5035,  0.4406,  0.5465, -0.3659,\n",
      "           0.7865,  1.3396,  0.2550, -1.3284, -0.2015, -0.6339,  1.2253,\n",
      "          -0.5040, -0.5530,  0.8649,  1.6294, -0.2354, -0.2679, -1.1126,\n",
      "          -1.3369, -0.7693,  0.0823, -0.5919,  0.2635,  1.0175, -2.2416,\n",
      "          -0.4478, -0.2539, -0.5223, -0.1861, -0.5383,  0.0480,  0.7780,\n",
      "          -2.2091,  0.6249, -0.0184,  1.0208, -1.7839, -2.8580,  1.6597,\n",
      "           0.5635,  2.6983, -2.3533, -0.4312,  0.3046,  1.8097, -1.9663,\n",
      "          -2.9021,  1.0197,  0.2381, -0.9045, -0.4091, -0.7331,  1.0546,\n",
      "           1.9870,  1.6220, -1.7440,  1.4611,  1.3275, -1.3674, -0.0866,\n",
      "          -0.3871],\n",
      "         [ 0.9014,  0.4834,  1.2908,  3.1409,  1.4169,  0.2047, -1.6492,\n",
      "          -3.0161, -2.1306,  1.3356, -0.3599,  3.4450,  0.9217, -1.1877,\n",
      "           0.8955,  0.0884, -1.7396, -0.1707, -2.7878, -1.1471,  2.0966,\n",
      "           0.1726,  1.0728,  1.0450,  1.1079,  1.6987,  2.6013, -2.1498,\n",
      "          -1.0534, -0.6450, -0.2374, -2.3821,  2.1092, -1.5330, -0.3256,\n",
      "          -0.6640, -3.5375,  0.0837, -0.3197,  0.2158,  0.3638, -1.1198,\n",
      "           0.3456,  2.2595, -1.0344,  0.7752, -0.3429,  1.2367, -0.2673,\n",
      "          -0.5816, -0.2201,  2.2850, -0.1111, -2.0580, -0.1376, -1.5985,\n",
      "          -0.8975, -0.5266, -0.2864, -2.7819, -0.3794, -0.1266,  3.3669,\n",
      "           1.5445],\n",
      "         [ 1.5167,  0.2498,  0.6130,  0.5193, -0.3553,  0.2472,  1.0757,\n",
      "           0.2755, -1.2974, -0.9623,  1.8181,  0.1319, -0.9675, -0.2754,\n",
      "           0.1282,  0.4822, -0.8074,  1.6875, -1.3877,  0.8586,  1.5858,\n",
      "           1.0251,  0.2758,  0.2066,  1.2984, -0.3388,  0.2546,  2.8690,\n",
      "           0.7813, -0.2860, -0.4321,  0.5702, -0.6299,  0.4813, -0.0659,\n",
      "          -1.4271,  1.3874, -1.1852,  0.8001,  0.8712, -0.0733, -0.3773,\n",
      "           0.7026,  0.4202, -0.5121, -2.7304, -1.8134,  1.5383, -0.2881,\n",
      "           1.2645, -0.7851,  1.9313, -0.1063,  0.8140, -1.0399,  0.9626,\n",
      "          -0.6439, -1.5716,  0.6517,  1.0364, -2.4138, -1.2121, -0.2441,\n",
      "           1.0173],\n",
      "         [ 0.1400, -1.5492, -1.4387, -1.7995,  1.0498,  1.4194, -0.5641,\n",
      "          -0.8286,  0.6730,  2.1540, -0.5770,  0.0441, -0.8729,  0.9716,\n",
      "           1.5790,  2.9093,  0.0170, -1.1538, -0.4973,  2.9965, -0.4816,\n",
      "          -1.5371, -0.7374, -1.3489,  1.0564, -0.9827, -0.9454,  2.4662,\n",
      "          -2.3932,  0.8371, -0.6401,  1.1220, -1.3741, -1.3623,  0.6077,\n",
      "           1.0528,  3.6033, -2.3686, -1.2878, -0.1665,  1.0018,  1.2158,\n",
      "           0.0499, -0.4499, -1.0223,  3.3915, -0.6519,  0.8433, -0.8385,\n",
      "          -1.2152,  0.4484, -0.5804,  1.0130, -0.2614, -1.4074,  0.3029,\n",
      "          -2.4561,  1.7189,  1.7135,  0.0440,  0.1106, -0.1659,  1.0587,\n",
      "          -0.8832],\n",
      "         [ 2.2280,  1.1983,  0.2951, -2.6059,  2.0126, -1.3212, -1.8948,\n",
      "          -3.1164,  0.1018,  1.9742,  0.4740, -2.2552,  0.8016, -1.0751,\n",
      "           0.8881,  2.0143, -0.7004,  0.4284,  0.0910,  0.3415, -0.8061,\n",
      "           1.1579,  1.6553, -1.2571, -0.8112, -1.1687,  0.2113,  4.0572,\n",
      "           1.5572,  1.8331,  0.6257,  1.7858, -0.8649, -0.5731,  0.2452,\n",
      "          -1.5786,  0.9605,  0.2225, -1.4866, -1.8262, -1.4645, -0.1890,\n",
      "          -1.0521, -0.4485, -2.1022,  0.6802,  1.6467,  1.6022, -2.2380,\n",
      "          -1.2869, -0.8395, -2.8442,  1.1676,  2.5047,  3.4187, -0.3338,\n",
      "          -0.4510,  1.5214,  1.9182,  1.2969, -0.9314, -1.0890, -2.1873,\n",
      "          -0.0519],\n",
      "         [ 0.8191,  2.6040,  1.4485, -1.4573,  1.1591, -3.3060,  1.1788,\n",
      "          -0.2317, -2.7073, -1.6255, -0.8196,  1.7351, -1.5957, -1.0859,\n",
      "           1.5788,  0.9615, -0.7969,  0.5560, -0.6131, -2.3218,  2.9438,\n",
      "           2.0033,  2.0809,  1.0204,  0.8569, -0.3976,  3.8788, -2.2100,\n",
      "          -0.6629,  0.5956,  2.3785, -0.9211,  1.2835,  0.2114, -0.5052,\n",
      "          -0.5144,  0.4671, -0.1826, -0.7034,  0.9458,  0.4039, -0.6184,\n",
      "          -1.1315, -0.9162,  2.6478,  0.3946, -0.6740, -1.8673,  2.2212,\n",
      "          -0.0582,  0.2384, -0.3550,  2.0018,  0.7683,  1.1857,  0.1937,\n",
      "           2.0054,  0.4336,  1.3451,  1.0096, -1.4454, -2.3497,  0.9135,\n",
      "          -2.8796]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# usign and testing \n",
    "\n",
    "text = \"नमस्ते\"\n",
    "\n",
    "# 1 create tokenizer \n",
    "tokenizer = Tokenizer(text)\n",
    "print(f\"Vocab size :{tokenizer.vocab_size}\")\n",
    "print(f\"chars :{tokenizer.chars}\")\n",
    "\n",
    "# 2. encode text \n",
    "test_text = \"नमस्ते\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "print(f\"Text: {test_text}--> Numbers{encoded}\" )\n",
    "\n",
    "\n",
    "# decode \n",
    "num = [1, 2, 3, 5, 0, 4]\n",
    "decoded = tokenizer.decode(num)\n",
    "print(f\"Number: {num}--> text : {decoded}\" )\n",
    "\n",
    "# create embedding layer \n",
    "embedding_dim = 64\n",
    "max_seq_len = 128 \n",
    "\n",
    "nepali_embedding = NepaliEmbedding(\n",
    "    vocab_size = tokenizer.vocab_size,\n",
    "    embedding_dim  = embedding_dim,\n",
    "    max_seq_len = max_seq_len\n",
    ")\n",
    "\n",
    "print(nepali_embedding)\n",
    "\n",
    "\n",
    "# 5. Process text through embedding \n",
    "indices_tensor = torch.tensor([encoded]) # convert to tensor \n",
    "print(f\"Input shape: {indices_tensor.shape}\")\n",
    "\n",
    "output = nepali_embedding(indices_tensor)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output:{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1591c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6984a9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
