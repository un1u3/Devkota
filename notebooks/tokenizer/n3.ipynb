{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847b9e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngoal : train SentencePiece tokenizer using BPE (subword tokenizer)\"\\n\\nO/P \\nartifacts/apm/nepali_bpe.model \\nartifacts/spm/nepali_bpe.vocab \\n\\nWHY BPE? \\n- Standard Chosice for gpt-style LM \\n- ig it should work well for Nepali \\n- handles rare words by splitting into subwords \\n\\nSpecial tokens (fixed IDS)\\nunk = 0  Unknown token\\nbos = 1  Beginning Of Sequence\\neos = 2  End Of Sequence\\npad = 3  Padding token\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "goal : train SentencePiece tokenizer using BPE (subword tokenizer)\"\n",
    "\n",
    "O/P \n",
    "artifacts/apm/nepali_bpe.model \n",
    "artifacts/spm/nepali_bpe.vocab \n",
    "\n",
    "WHY BPE? \n",
    "- Standard Chosice for gpt-style LM \n",
    "- ig it should work well for Nepali \n",
    "- handles rare words by splitting into subwords \n",
    "\n",
    "Special tokens (fixed IDS)\n",
    "unk = 0  Unknown token\n",
    "bos = 1  Beginning Of Sequence\n",
    "eos = 2  End Of Sequence\n",
    "pad = 3  Padding token\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee24d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import sentencepiece as spm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58b62ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(train_txt_path, model_prefix, vocab_size):\n",
    "    model_prefix.parent.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        input = str(train_txt_path),\n",
    "        model_prefix=str(model_prefix),\n",
    "        model_type = \"bpe\",\n",
    "        vocab_size = vocab_size,\n",
    "        character_coverage = 1.0,\n",
    "\n",
    "    # my device is slow so \n",
    "    shuffle_input_sentence = True,\n",
    "    input_sentence_size = 200000,\n",
    "    train_extremely_large_corpus = True,\n",
    "\n",
    "    # speicals token IDS \n",
    "    unk_id=0,\n",
    "        bos_id=1,\n",
    "        eos_id=2,\n",
    "        pad_id=3,\n",
    "\n",
    "    )\n",
    "    print('Tokenzier Trained')\n",
    "    print(\"Model :\", str(model_prefix) + \".model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e0e84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../data/preprocessed/input.txt\n",
      "  input_format: \n",
      "  model_prefix: artifacts/spm/nepali_bpe16k\n",
      "  model_type: BPE\n",
      "  vocab_size: 16000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 200000\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 1\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 3\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  â‡ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: ../data/preprocessed/input.txt\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Not found: \"../data/preprocessed/input.txt\": No such file or directory Error #2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_bpe(\n\u001b[1;32m      2\u001b[0m     train_txt_path\u001b[38;5;241m=\u001b[39mPath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/preprocessed/input.txt\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      3\u001b[0m     model_prefix\u001b[38;5;241m=\u001b[39mPath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124martifacts/spm/nepali_bpe16k\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      4\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m\n\u001b[1;32m      5\u001b[0m )\n",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m, in \u001b[0;36mtrain_bpe\u001b[0;34m(train_txt_path, model_prefix, vocab_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_bpe\u001b[39m(train_txt_path, model_prefix, vocab_size):\n\u001b[1;32m      2\u001b[0m     model_prefix\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m     spm\u001b[38;5;241m.\u001b[39mSentencePieceTrainer\u001b[38;5;241m.\u001b[39mTrain(\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(train_txt_path),\n\u001b[1;32m      6\u001b[0m         model_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(model_prefix),\n\u001b[1;32m      7\u001b[0m         model_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbpe\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m         vocab_size \u001b[38;5;241m=\u001b[39m vocab_size,\n\u001b[1;32m      9\u001b[0m         character_coverage \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# my device is slow so \u001b[39;00m\n\u001b[1;32m     12\u001b[0m     shuffle_input_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m     input_sentence_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200000\u001b[39m,\n\u001b[1;32m     14\u001b[0m     train_extremely_large_corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# speicals token IDS \u001b[39;00m\n\u001b[1;32m     17\u001b[0m     unk_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     18\u001b[0m         bos_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     19\u001b[0m         eos_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     20\u001b[0m         pad_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTokenzier Trained\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel :\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(model_prefix) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sentencepiece/__init__.py:1047\u001b[0m, in \u001b[0;36mSentencePieceTrainer.Train\u001b[0;34m(arg, logstream, **kwargs)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mTrain\u001b[39m(arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, logstream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1046\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m _LogStream(ostream\u001b[38;5;241m=\u001b[39mlogstream):\n\u001b[0;32m-> 1047\u001b[0m     SentencePieceTrainer\u001b[38;5;241m.\u001b[39m_Train(arg\u001b[38;5;241m=\u001b[39marg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sentencepiece/__init__.py:1040\u001b[0m, in \u001b[0;36mSentencePieceTrainer._Train\u001b[0;34m(arg, **kwargs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SentencePieceTrainer\u001b[38;5;241m.\u001b[39m_TrainFromMap2(new_kwargs, sentence_iterator)\n\u001b[1;32m   1039\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SentencePieceTrainer\u001b[38;5;241m.\u001b[39m_TrainFromMap(new_kwargs)\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sentencepiece/__init__.py:985\u001b[0m, in \u001b[0;36mSentencePieceTrainer._TrainFromMap\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_TrainFromMap\u001b[39m(args):\n\u001b[0;32m--> 985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _sentencepiece\u001b[38;5;241m.\u001b[39mSentencePieceTrainer__TrainFromMap(args)\n",
      "\u001b[0;31mOSError\u001b[0m: Not found: \"../data/preprocessed/input.txt\": No such file or directory Error #2"
     ]
    }
   ],
   "source": [
    "train_bpe(\n",
    "    train_txt_path=Path('../../data/preprocessed/input.txt'),\n",
    "    model_prefix=Path(\"artifacts/spm/nepali_bpe16k\"),\n",
    "    vocab_size=16000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685ccbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d66f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path('../../data/preprocessed/input.txt').exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a5689a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4db1b64e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e575122e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
