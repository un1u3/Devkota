{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73825155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-head attention \n",
    "\n",
    "# Attention(Q,K,V) = softmax(Q^T/ sqrt(d_k))V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73d20aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b9f09de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    # this  splits the d_model into num_heads ,\n",
    "    #  performs attention in parallel then concatenates and projects the reslt\n",
    "\n",
    "    def __init__(self, d_model, num_heads = 8 , dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model\n",
    "\n",
    "        #  linear projection for Q, K, V \n",
    "        # we use single linear layer, then split into heads \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # output projection \n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "\n",
    "\n",
    "        # scaling factor \n",
    "        self.scale = math.sqrt(self.d_k) \n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # split the last dim into (num_heads, d_k)\n",
    "\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "\n",
    "        # reshape to (batchsize, seqlen, num_heads, dk)\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        return x.transpose(1,2)\n",
    "    \n",
    "    def combine_heads(self,x):\n",
    "        # inverse of split heads \n",
    "        batch_size, num_heads, seq_len, d_k = x.size()\n",
    "\n",
    "        # idk what contiguous does fk it \n",
    "        x = x.transpose(1,2).contiguous()\n",
    "\n",
    "    def scaled_dot_prod_attn(self, Q, K, V, mask = None):\n",
    "        # attn score \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1))  / self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask==0, float('-inf'))\n",
    "        \n",
    "        # applyign softmax to get aattn weights \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # appy dropout \n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        return output, attn_weights\n",
    "    \n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        batch_size = query.size(0)\n",
    "        # Linear projection \n",
    "        Q = self.W_q(query) # (batch_size, seq_len_q, d_model)\n",
    "        K = self.W_k(key)    \n",
    "        V = self.W_v(value)  \n",
    "        \n",
    "        # Split into multiple heads\n",
    "        Q = self.split_heads(Q)  # (batch_size, num_heads, seq_len_q, d_k)\n",
    "        K = self.split_heads(K)  \n",
    "        V = self.split_heads(V)  \n",
    "\n",
    "        attn_output, _ = self.scaled_dot_prod_attn(Q, K, V, mask)\n",
    "\n",
    "        # combine heads \n",
    "        attn_output = self.combine_heads(attn_output)\n",
    "\n",
    "        output = self.W_o(attn_output)\n",
    "        return output\n",
    "    \n",
    "    def create_casual_mask(seq_len, device):\n",
    "        # Create a casual (lower traingular) mask for autoreg gen\n",
    "        # ovi lower traingular matrix \n",
    "        # this prevnets position from attending futrure pred \n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "        return mask \n",
    "    \n",
    "\n",
    "    def create_padding_mask(seq, pad_idx= 3):\n",
    "        mask = (seq != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        return mask\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aece1dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
