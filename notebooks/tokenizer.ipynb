{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a265a7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning \n",
    "\n",
    "import re \n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from pathlib import Path \n",
    "import sentencepiece as spm \n",
    "\n",
    "\n",
    "\n",
    "class CorpusBuilder:\n",
    "    # cleans and prepares text dat for toekniier trainng \n",
    "    # what it does \n",
    "    #  remvoes junk char \n",
    "    # keeps only good line \n",
    "\n",
    "\n",
    "    def __init__(self, min_chars=5):\n",
    "        self.min_chars = min_chars\n",
    "        self.control_chars = re.compile(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f]\")\n",
    "        self.multiple_spaces = re.compile(r\"[ \\t\\u00A0]+\")\n",
    "\n",
    "    def clean_line(self, text):\n",
    "        # zero width space \n",
    "        text = text.replace('\\u200b',\"\")\n",
    "        # remove bom byte oreder mark k\n",
    "        text = text.replace(\"\\uffeff\",\"\")\n",
    "        # remove contril characters \n",
    "        text = self.control_chars.sub(\"\",text)\n",
    "        # remove extraa spaces at start/en \n",
    "        text = text.strip()\n",
    "        return text \n",
    "    \n",
    "\n",
    "    # def build_corpus(self, input_files, output_file):\n",
    "    #     # c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea84521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer trainer \n",
    "\n",
    "class TokenizerTrainer:\n",
    "    # trains a SP tokenizer on your corpus \n",
    "    def __init__(self, vocab_size = 16000):\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def train(self, corpus_file, output_dir, model_name=\"tokenizer\"):\n",
    "\n",
    "        # create op direc\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # full path for model \n",
    "        model_prefix = output_path / model_name\n",
    "\n",
    "        print(\"trining.........\")\n",
    "        print(\"-\"*69)\n",
    "        print(f\"corpus :{corpus_file}\")\n",
    "        print(f\"Vocab_size: {self.vocab_size}\")\n",
    "        print(f\"model op:{model_prefix}.model\")\n",
    "\n",
    "\n",
    "        spm.SentencePieceTrainer.Train(\n",
    "            input = str(corpus_file),\n",
    "            model_prefix = str(model_prefix),\n",
    "\n",
    "            # setting \n",
    "            model_type = \"bpe\", # tying bpe at first might change latter\n",
    "            vocab_size = self.vocab_size,\n",
    "            character_coverage  = 0.9995, #chtgpt said to put this \n",
    "\n",
    "            # special toksn (fixed)\n",
    "            unk_id=0,  # Unknown token\n",
    "            bos_id=1,  # Beginning of sequence\n",
    "            eos_id=2,  # End of sequence  \n",
    "            pad_id=3,  # Padding token\n",
    "\n",
    "            # performance (specially for large data)\n",
    "            shuffle_input_sentence=True,\n",
    "            input_sentence_size=2000000,\n",
    "            train_extremely_large_corpus=False,  \n",
    "\n",
    "        )\n",
    "\n",
    "        print(\"-\"*69)\n",
    "        print(\"Done traininng\")\n",
    "        print(\"Model Saved\")\n",
    "        print(\"vocab saved\")\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f36b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer*( for using the trained model)\n",
    "\n",
    "class NepaliTokenizer:\n",
    "\n",
    "    def __init__(self, model_path):\n",
    "        # load a train tokenizer \n",
    "\n",
    "        # checking if file exists \n",
    "        if not Path(model_path).exists():\n",
    "            raise FileExistsError(f\"FIle xina babu:{model_path}\")\n",
    "        \n",
    "        # load spm model \n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        sucess = self.sp.Load(str(model_path))\n",
    "\n",
    "        if not sucess:\n",
    "            raise RuntimeError(f\"Failed to laod model\")\n",
    "        \n",
    "        # get special token IDS \n",
    "        self.unk_id = self.sp.unk_id()\n",
    "        self.bos_id = self.sp.bos_id()\n",
    "        self.eos_id = self.sp.eos_id()\n",
    "        self.pad_id = self.sp.pad_id()\n",
    "\n",
    "        print(\"TOkeinzer loaded\")\n",
    "        print(\"Vocab size: {self.vocab_size}\")\n",
    "\n",
    "    def vocab_size(self):\n",
    "        return self.sp.GetPieceSize()\n",
    "    \n",
    "    def encode(self, text, add_bos= False, add_eos = False):\n",
    "        # convert text to token ids \n",
    "        # text  Input : नमस्ते\n",
    "        # add bos : add begnign token at stat \n",
    "        # add_eos : add end token at end\n",
    "        # returns  token idss\n",
    "\n",
    "        # get token ids from sp \n",
    "        ids = self.sp.EncodeAsIds(text)\n",
    "        \n",
    "        # Build result\n",
    "        result = []\n",
    "        \n",
    "        # Add beginning token if requested\n",
    "        if add_bos:\n",
    "            result.append(self.bos_id)\n",
    "        \n",
    "        # Add all token IDs\n",
    "        for token_id in ids:\n",
    "            result.append(token_id)\n",
    "        \n",
    "        # Add end token if requested\n",
    "        if add_eos:\n",
    "            result.append(self.eos_id)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "\n",
    "    def decode(self, ids, remove_special = True):\n",
    "        # vice versa of encodder \n",
    "        # filter specal tokens if requurested \n",
    "        if remove_special:\n",
    "            filtered = []\n",
    "            for token_id in ids:\n",
    "                # skipping special tokens \n",
    "                if token_id == self.bos_id:\n",
    "                    continue\n",
    "                if token_id == self.eos_id:\n",
    "                    continue\n",
    "                if token_id == self.pad_id:\n",
    "                    continue\n",
    "\n",
    "                # keeping regular tokens\n",
    "                filtered.append(token_id)\n",
    "            ids = filtered\n",
    "\n",
    "        # decode to text \n",
    "        return self.sp.DecodeIds(ids)\n",
    "    \n",
    "    def encode_batch(self, texts, add_bos=False, add_eos=False):\n",
    "        \n",
    "        # Encode multiple texts at once.\n",
    "        \n",
    "        results = []\n",
    "        for text in texts:\n",
    "            encoded = self.encode(text, add_bos=add_bos, add_eos=add_eos)\n",
    "            results.append(encoded)\n",
    "        return results\n",
    "    \n",
    "    def decode_batch(self, ids_list, remove_special=True):\n",
    "       \n",
    "        # Decode multiple token sequences at once.\n",
    "        results = []\n",
    "        for ids in ids_list:\n",
    "            decoded = self.decode(ids, remove_special=remove_special)\n",
    "            results.append(decoded)\n",
    "              \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c30efd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trining.........\n",
      "---------------------------------------------------------------------\n",
      "corpus :sample_nepali_corpus.txt\n",
      "Vocab_size: 100\n",
      "model op:tokenizer_test/nepali_bpe.model\n",
      "---------------------------------------------------------------------\n",
      "Done traininng\n",
      "Model Saved\n",
      "vocab saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: sample_nepali_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: tokenizer_test/nepali_bpe\n",
      "  model_type: BPE\n",
      "  vocab_size: 100\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 2000000\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 3\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: sample_nepali_corpus.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 3 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=101\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=27\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 3 sentences.\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 3\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 17\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=20 all=82 active=55 piece=का\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=40 all=88 active=61 piece=ाखमा\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=60 all=71 active=44 piece=मे\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: tokenizer_test/nepali_bpe.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: tokenizer_test/nepali_bpe.vocab\n"
     ]
    }
   ],
   "source": [
    "trainer = TokenizerTrainer(vocab_size=100)\n",
    "trainer.train(corpus_file=corpus_path, output_dir=\"tokenizer_test\", model_name=\"nepali_bpe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c363194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOkeinzer loaded\n",
      "Vocab size: {self.vocab_size}\n",
      "Encoded IDs: [1, 21, 22, 19, 8, 2]\n",
      "Decoded text: जीवन कविता हो ।\n"
     ]
    }
   ],
   "source": [
    "tokenizer = NepaliTokenizer(\"tokenizer_test/nepali_bpe.model\")\n",
    "\n",
    "text = \"जीवन कविता हो ।\"\n",
    "\n",
    "ids = tokenizer.encode(text, add_bos=True, add_eos=True)\n",
    "print(\"Encoded IDs:\", ids)\n",
    "\n",
    "decoded = tokenizer.decode(ids)\n",
    "print(\"Decoded text:\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b0aa6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch encoded: [[1, 16, 38, 31, 5, 73, 0, 85, 95, 99, 8, 2], [1, 22, 47, 49, 19, 8, 2]]\n",
      "Batch decoded: None\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"म पहाडमा बस्छु ।\",\n",
    "    \"कविता मेरो आत्मा हो ।\"\n",
    "]\n",
    "\n",
    "encoded_batch = tokenizer.encode_batch(texts, add_bos=True, add_eos=True)\n",
    "print(\"Batch encoded:\", encoded_batch)\n",
    "\n",
    "decoded_batch = tokenizer.decode_batch(encoded_batch)\n",
    "print(\"Batch decoded:\", decoded_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc07f992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
