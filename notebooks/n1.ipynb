{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e29845e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FRIDAY 1: TOKENIZATION & EMBEDDINGS \n",
    "# components \n",
    "# 1. Tokenizer Convertes text <-> Numbers\n",
    "# 2. Token Embedding - Numbers -> Dense vectors \n",
    "# 3. Positional embeddings - Add position information \n",
    "# 4. Testing & Visualization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52a0ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import os \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86823e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्कार! आज म बिहान ७:३० बजे उठेँ। मौसम सुन्दर छ—घाम चम्किरहेको छ। मैले चिया बनाएँ र समाचार पढें। स्कूल जानुअघि किताबहरू (गणित, विज्ञान, साहित्य) र कापी, कलम, पेन्सिल तयारी गर्नुपर्नेछ। हिजोका कार्यहरू, ईमेलहरू, र फेसबुक/इन्स्टाग्राम जाँच्न पनि समय लिनुपर्\\u200dयो। मैले सँगै @राम, #सुनिता, र मित्रहरूसँग कुरा गरें। उनीहरूले भने: “हामी क्याफेमा १०:०० बजे भेट्नेछौं।” त्यसपछि म घर फर्केर टेलिफोन नम्बर ०१-४४५५६७८९ मा सम्पर्क गरें। आजको दिन धेरै व्यस्त छ; तर काम पूरा गर्दा खुशी लाग्छ।'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data \n",
    "\n",
    "with open('../data/sample.txt','r',encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0036d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# component 1 : TOKENIZER \n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Character -Level tokenizer for Nepali text \n",
    "    what it does \n",
    "    - Builds a vocabulart for all uqnieu characters in the text\n",
    "    - Convert text to numbers(encoding) and vice versa \n",
    "    \"\"\"\n",
    "    def __init__(self,text):\n",
    "        # process : extract->sort->create bidirectional mapping\n",
    "\n",
    "        # 1. get all unique characterrs from the text\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        # 2. count how many unqiue characters from the text \n",
    "        self.vocab_size = len(self.chars)\n",
    "        # encoding \n",
    "        self.stoi = {} # start withh an empty dictionary \n",
    "        for i,ch in enumerate(self.chars): # go through each character with index \n",
    "            self.stoi[ch] = i # assign the index t the character \n",
    "        \n",
    "        # decoding \n",
    "        self.itos = {}\n",
    "        for i,ch in enumerate(self.chars):\n",
    "            self.itos[i] = ch \n",
    "        \n",
    "    def encode(self,text):\n",
    "        indices = []\n",
    "        for c in text:\n",
    "            indices.append(self.stoi[c])\n",
    "        return indices\n",
    "\n",
    "    def decode(self,indices):\n",
    "        chars = []\n",
    "        for i in indices:\n",
    "            chars.append(self.itos[i])\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8217fb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्कार! आज म बिहान ७:३० बजे उठेँ। मौसम सुन्दर छ—घाम चम्किरहेको छ। मैले चिया बनाएँ र समाचार पढें। स्कूल जानुअघि किताबहरू (गणित, विज्ञान, साहित्य) र कापी, कलम, पेन्सिल तयारी गर्नुपर्नेछ। हिजोका कार्यहरू, ईमेलहरू, र फेसबुक/इन्स्टाग्राम जाँच्न पनि समय लिनुपर्\\u200dयो। मैले सँगै @राम, #सुनिता, र मित्रहरूसँग कुरा गरें। उनीहरूले भने: “हामी क्याफेमा १०:०० बजे भेट्नेछौं।” त्यसपछि म घर फर्केर टेलिफोन नम्बर ०१-४४५५६७८९ मा सम्पर्क गरें। आजको दिन धेरै व्यस्त छ; तर काम पूरा गर्दा खुशी लाग्छ।'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eg usages \n",
    "tokenizer = Tokenizer(text)\n",
    "token = tokenizer.encode(text)\n",
    "tokenizer.decode(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "180de271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# component 2: TOKEN EMBEDINGS \n",
    "class TokenEmbedding(nn.Module):\n",
    "    # Converts token indices to dense vector representaions.\n",
    "    # What it does \n",
    "        # - each character index -> D-dimenssional vector \n",
    "        # - vectors are learned during training \n",
    "        # - similar characters get similar vectors \n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        # args\n",
    "        # vocab_size = num of unique tokens(chars)\n",
    "        # embedding_dim : size of embedding vectors \n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # creates the embedding layer \n",
    "        # this is essentially a lookup table (matrix)\n",
    "\n",
    "\n",
    "    def forward(self, token_indices):\n",
    "        return self.embedding(token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eeeb5a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmponent 3 : POSITIONAL EMBedDINGS\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\" \n",
    "    adds position information to token embeddings.\n",
    "    why wee nedd this:\n",
    "        - transformers process all tokens in parallel \n",
    "        - without position info, \"hello world\" == \"world hello\"\n",
    "        - position embeddings tell model the order of characters \n",
    "\n",
    "    two approaches \n",
    "    1. Learned(what we use ): Position embeddings are trainable parameteres \n",
    "    2. Fixed (Sinusoidal) : Use sin/cos functions\n",
    "    each position gets a unqiue learned vector added to it\n",
    "    \"\"\"\n",
    "    def __init__(self, max_seq_len, embedding_dim):\n",
    "        # creates position mebedding matrix of shape( max_seq_len, embedding_dim)\n",
    "        # one learned vector for each possibility \n",
    "        super().__init__()\n",
    "        # store dimensions \n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "\n",
    "\n",
    "    def forward(self, token_embeddings):\n",
    "        #  1. Get sequence length from input\n",
    "        #  2. Create position indices: [0, 1, 2, ..., seq_len-1]\n",
    "        #  3. Look up position embeddings\n",
    "        #  4. Add to token embeddings (broadcasting handles batch dimension)\n",
    "\n",
    "    # check : sequence can't be longer than max_seq_len\n",
    "        if seq_len > self.max_seq_len:\n",
    "            raise ValueError(\n",
    "                f\"Sequence Length {seq_len} exceeds maximum {self.max_seq_len}\"\n",
    "            )\n",
    "        # device= ensures positions are on same device as input (CPU/GPU)\n",
    "        positions = torch.arange(seq_len, device = token_embeddings.device)\n",
    "        # Look up position embeddings\n",
    "        # Shape: (seq_len, embedding_dim)\n",
    "        pos_emb = self.embedding(positions)\n",
    "\n",
    "\n",
    "        # Add position embeddings to token embeddings\n",
    "        # Broadcasting: pos_emb (seq_len, emb_dim) is added to each batch\n",
    "        # Result shape: (batch_size, seq_len, embedding_dim)\n",
    "        return token_embeddings + pos_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d208ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# component  4: COMBINED EMBEDDING LAYER\n",
    "\n",
    "class NepaliEmbedding(nn.Module):\n",
    "    # token + position \n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len):\n",
    "        # Initialize combined embedding layer.\n",
    "        super().__init__()\n",
    "\n",
    "        # store configuration \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim \n",
    "        self.max_seq_len = max_seq_len \n",
    "\n",
    "        # create token embeding layer \n",
    "        self.token_emb = TokenEmbedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # create positiona embedding layer \n",
    "        self.pos_emb  = PositionalEmbedding(max_seq_len, embedding_dim)\n",
    "\n",
    "        # calcualate total paramteres \n",
    "        total_parms = (vocab_size * embedding_dim) + (max_seq_len * embedding_dim)\n",
    "    \n",
    "    def forward(self, token_indices):\n",
    "        # convert tokenn indices to embeddings with postion embedding \n",
    "\n",
    "        # 1. get token embedding \n",
    "        tok_emb = self.token_emb(token_indices)\n",
    "\n",
    "        # 2.Add positioal embedding\n",
    "        embeddings = self.pos_emb(tok_emb)\n",
    "        return embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38f32aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional embedding output shape: torch.Size([1, 128, 32])\n"
     ]
    }
   ],
   "source": [
    "# Fix PositionalEmbedding.forward (seq_len was not defined) and replace the model's pos_emb\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_seq_len, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "\n",
    "    def forward(self, token_embeddings):\n",
    "        # token_embeddings: (batch_size, seq_len, embedding_dim) or (seq_len, embedding_dim)\n",
    "        if token_embeddings.dim() == 3:\n",
    "            seq_len = token_embeddings.size(1)\n",
    "        elif token_embeddings.dim() == 2:\n",
    "            seq_len = token_embeddings.size(0)\n",
    "            token_embeddings = token_embeddings.unsqueeze(0)  # add batch dim for consistent addition\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected token_embeddings shape: {token_embeddings.shape}\")\n",
    "\n",
    "        if seq_len > self.max_seq_len:\n",
    "            raise ValueError(\n",
    "                f\"Sequence Length {seq_len} exceeds maximum {self.max_seq_len}\"\n",
    "            )\n",
    "\n",
    "        positions = torch.arange(seq_len, device=token_embeddings.device)\n",
    "        pos_emb = self.embedding(positions)  # (seq_len, embedding_dim)\n",
    "\n",
    "        # broadcasting will add pos_emb (seq_len, emb_dim) to token_embeddings (batch, seq_len, emb_dim)\n",
    "        return token_embeddings + pos_emb\n",
    "\n",
    "# replace existing positional embedding in the existing model instance\n",
    "embedding.pos_emb = PositionalEmbedding(embedding.max_seq_len, embedding.embedding_dim)\n",
    "\n",
    "# smoke test\n",
    "tok_emb = embedding.token_emb(indices_tensor)\n",
    "out = embedding.pos_emb(tok_emb)\n",
    "print(\"positional embedding output shape:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f04e1af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1591c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6984a9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
